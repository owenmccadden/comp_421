{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Setup\n",
    "- I copied the sql into a file sql_dump.txt and ran each line to setup the spam table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"bromberg\",\n",
    "        database=\"predict_spam\"\n",
    ")\n",
    "\n",
    "sql_dump = open('sql_dump.txt', 'r')\n",
    "lines = sql_dump.readlines()\n",
    "sql_dump.close()\n",
    "for line in lines:\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining List of Spammy Words\n",
    "- I counted the frequencies of all the unique words for spam and ham messages\n",
    "- I selected the top ten spammy words that appeared most frequently in spam messages that did not occur frequently in ham messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Words,  Ham Words\n",
      "(('to', 606), ('to', 1531))\n",
      "(('a', 358), ('you', 1485))\n",
      "(('your', 187), ('I', 1444))\n",
      "(('or', 185), ('the', 1021))\n",
      "(('call', 185), ('a', 969))\n",
      "(('the', 178), ('', 911))\n",
      "(('for', 169), ('in', 738))\n",
      "(('2', 169), ('and', 737))\n",
      "(('you', 164), ('i', 736))\n",
      "(('is', 145), ('u', 663))\n",
      "(('Call', 136), ('is', 640))\n",
      "(('on', 135), ('my', 618))\n",
      "(('have', 128), ('me', 556))\n",
      "(('and', 119), ('of', 500))\n",
      "(('from', 116), ('for', 477))\n",
      "(('ur', 107), ('that', 398))\n",
      "(('with', 101), ('it', 388))\n",
      "(('&', 98), ('your', 374))\n",
      "(('of', 93), ('on', 353))\n",
      "(('4', 93), ('have', 347))\n",
      "(('FREE', 89), ('at', 336))\n",
      "(('mobile', 83), ('not', 326))\n",
      "(('are', 77), ('are', 319))\n",
      "(('You', 77), ('be', 314))\n",
      "(('our', 76), ('will', 291))\n",
      "(('claim', 73), ('get', 286))\n",
      "(('To', 73), ('2', 286))\n",
      "(('Your', 71), ('can', 281))\n",
      "(('U', 70), (\"I'm\", 278))\n",
      "(('text', 69), ('but', 275))\n",
      "(('txt', 68), ('so', 273))\n",
      "(('in', 66), ('with', 250))\n",
      "(('now', 64), ('U', 239))\n",
      "(('Txt', 63), ('do', 237))\n",
      "(('reply', 58), ('up', 231))\n",
      "(('contact', 56), ('when', 224))\n",
      "(('free', 56), ('.', 223))\n",
      "(('-', 55), ('go', 222))\n",
      "(('STOP', 50), ('if', 218))\n",
      "(('be', 50), ('like', 213))\n",
      "(('u', 49), ('we', 209))\n",
      "(('now!', 49), ('know', 205))\n",
      "(('who', 47), ('come', 200))\n",
      "(('just', 47), ('got', 200))\n",
      "(('this', 46), ('all', 193))\n",
      "(('send', 46), ('call', 191))\n",
      "(('Nokia', 45), ('was', 191))\n",
      "(('only', 45), ('just', 189))\n",
      "(('get', 45), ('You', 189))\n",
      "(('won', 45), ('or', 188))\n",
      "(('per', 44), ('ur', 186))\n",
      "(('prize', 44), ('this', 179))\n",
      "(('service', 43), ('am', 176))\n",
      "(('been', 43), ('out', 170))\n",
      "(('cash', 42), ('...', 162))\n",
      "(('new', 42), ('4', 157))\n",
      "(('Reply', 42), ('now', 151))\n",
      "(('500', 41), ('going', 148))\n",
      "(('out', 41), ('about', 146))\n",
      "(('stop', 39), ('want', 142))\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "spam_words = defaultdict(int)\n",
    "ham_words = defaultdict(int)\n",
    "\n",
    "sql = \"select * from spam;\"\n",
    "cursor = db.cursor()\n",
    "cursor.execute(sql)\n",
    "output = cursor.fetchall()\n",
    "\n",
    "for line in output:\n",
    "    if line[0] == 'spam':\n",
    "        for word in line[1].split(' '):\n",
    "            spam_words[word] += 1\n",
    "    if line[0] == 'ham':\n",
    "        for word in line[1].split(' '):\n",
    "            ham_words[word] += 1\n",
    "\n",
    "sorted_spam = sorted(spam_words.items(), key=lambda item: item[1])[::-1]\n",
    "sorted_ham = sorted(ham_words.items(), key=lambda item: item[1])[::-1]\n",
    "\n",
    "print('Spam Words,  Ham Words')\n",
    "for i in list(zip(sorted_spam, sorted_ham))[:60]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "spammy_words = ['call', 'free', 'mobile', 'claim', 'reply', 'contact', 'stop', 'won', 'prize', 'cash']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Features Class\n",
    "- this class is used to generate the features.csv file used to build the Weka decision tree diagram\n",
    "- the class is initialized with a reference to the database connection, and a dataframe\n",
    "- the class contains methods to compute the required features of each entry in the database\n",
    "- the buildCsv method iterates through each entry in the spam database, computes the required features, and stores the results in features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class ComputeFeatures:\n",
    "    def __init__(self, host=\"\", username=\"\", password=\"\", database=\"\"):\n",
    "        self.db = mysql.connector.connect(host=host, user=username, password=password, database=database)\n",
    "        self.df = pd.DataFrame()\n",
    "    \n",
    "    def doesHaveLinks(self, sms):\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        result = [x[0] for x in re.findall(regex, sms)]\n",
    "        return len(result) > 0\n",
    "    \n",
    "    def doesHaveSpammyWords(self, sms):\n",
    "        spammy_words = ['call', 'free', 'mobile', 'claim', 'reply', 'contact', 'stop', 'won', 'prize', 'cash']\n",
    "        for word in spammy_words:\n",
    "            if word in sms:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def lengthOfText(self, sms):\n",
    "        return len(sms)\n",
    "    \n",
    "    def buildCsv(self):\n",
    "        class_label = []\n",
    "        does_have_links = []\n",
    "        does_have_spammy_words = []\n",
    "        length_of_text = []\n",
    "        \n",
    "        sql = \"select * from spam\"\n",
    "        cursor = db.cursor()\n",
    "        cursor.execute(sql)\n",
    "        output = cursor.fetchall()\n",
    "        \n",
    "        for line in output:\n",
    "            classification = line[0]\n",
    "            sms = line[1]\n",
    "            class_label.append(classification)\n",
    "            does_have_links.append(self.doesHaveLinks(sms))\n",
    "            does_have_spammy_words.append(self.doesHaveSpammyWords(sms))\n",
    "            length_of_text.append(self.lengthOfText(sms))\n",
    "        self.df['Class Label'] = class_label\n",
    "        self.df['doesHaveLinks'] = does_have_links\n",
    "        self.df['doesHaveSpammyWords'] = does_have_spammy_words\n",
    "        self.df['lengthOfText'] = length_of_text\n",
    "        self.df.reset_index(drop=True)\n",
    "        self.df.to_csv('features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the ComputeFeatures class and creating features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = ComputeFeatures('localhost', 'root', 'bromberg', 'predict_spam')\n",
    "cf.buildCsv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weka Decision Tree\n",
    "![picture](https://drive.google.com/uc?id=1NX8XwJ83Ijwfd3CbayJ4GMqNPXYtXQML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Spam Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictSpam:\n",
    "    def __init__(self, cf):\n",
    "        self.cf = cf\n",
    "    \n",
    "    def makePrediction(self, sms):\n",
    "        hasLinks = self.cf.doesHaveLinks(sms)\n",
    "        hasSpammyWords = self.cf.doesHaveSpammyWords(sms)\n",
    "        lengthOfText = self.cf.lengthOfText(sms)\n",
    "        if lengthOfText > 97:\n",
    "            if hasSpammyWords:\n",
    "                if lengthOfText > 183:\n",
    "                    return 'ham'\n",
    "                else:\n",
    "                    if lengthOfText > 127:\n",
    "                        return 'spam'\n",
    "                    else:\n",
    "                        if hasLinks:\n",
    "                            return 'spam'\n",
    "                        else:\n",
    "                            return 'ham'\n",
    "            else:\n",
    "                if hasLinks:\n",
    "                    return 'spam'\n",
    "                else:\n",
    "                    if lengthOfText > 159:\n",
    "                        if lengthOfText > 161:\n",
    "                            return 'ham'\n",
    "                        else:\n",
    "                            return 'spam'\n",
    "                    else:\n",
    "                        return 'ham'\n",
    "        else:\n",
    "            if hasLinks:\n",
    "                return 'spam'\n",
    "            else:\n",
    "                return 'ham'\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Predict Spam Class\n",
    "- here I initialize a makePrediction object using the Compute Features object\n",
    "- I then test the output of the makePrediction function with every entry in the database and compare the results to measure the accuracy of the prediction model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model on given database of 5572 messages: 93.0%\n"
     ]
    }
   ],
   "source": [
    "ps = predictSpam(cf)\n",
    "\n",
    "sql = \"select * from spam;\"\n",
    "cursor = db.cursor()\n",
    "cursor.execute(sql)\n",
    "output = cursor.fetchall()\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for line in output:\n",
    "    e = ps.makePrediction(line[1])\n",
    "    a = line[0]\n",
    "    if e == a:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy of model on given database of 5572 messages: {}%'.format(round(correct/len(output), 2) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "- After running all the entries in the database through the prediction model, I found that the model is about 93% accurate. This matches the results I saw in Weka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
